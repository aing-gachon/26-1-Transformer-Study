{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5278d77c",
   "metadata": {},
   "source": [
    "# Transformer from scratch — Blank Notebook (Aing)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aing-gachon/26-Spring-Transformer-Study/blob/main/Week2/Aing_Transformer_from_scratch_blank.ipynb)\n",
    "\n",
    "이 노트북은 `transformer_from_scratch.py`를 기반으로 **Transformer 핵심 구성요소**(Self-Attention / Multi-Head / Add&Norm / FFN / Encoder-Decoder / Mask)를  \n",
    "`____` **빈칸을 채우며** 논문 **Eq.(1), Eq.(2), Fig.2**의 흐름을 코드로 연결하는 실습 자료입니다.\n",
    "\n",
    "- 빈칸은 **이해에 핵심인 지점만** 뚫었습니다. (의미 없는 빈칸 X)\n",
    "- 빈칸 옆 주석은 **정답을 그대로 복붙할 수 없도록**, “역할/의도” 중심으로만 적었습니다.\n",
    "- 아래 **실습 코드(학습 루프)** 는 제공된 그대로 사용하며, **그 부분은 빈칸이 없습니다.**\n",
    "\n",
    "> 사용법  \n",
    "> 1) 위에서부터 내려오며 `____`만 채우기  \n",
    "> 2) Shape 주석과 동일한지 수시로 확인하기  \n",
    "> 3) 마지막 `__main__` 테스트로 end-to-end 동작 확인하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382bd87e",
   "metadata": {},
   "source": [
    "## 실행을 위한 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c256a4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# 필수: PyTorch (텐서 연산/모델 정의)\n",
    "!pip install torch\n",
    "!pip -q install spacy datasets sacrebleu\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e872e421",
   "metadata": {},
   "source": [
    "## 학습 목표\n",
    "\n",
    "1. **Scaled Dot-Product Attention (Eq.1)** 을 코드로 구현하고, $QK^T / sqrt(d_k)$의 의미를 설명할 수 있다.  \n",
    "2. **Multi-Head Attention**에서 `(N, seq, d_model)` → `(N, heads, seq, d_k)`로의 변환을 직접 구현할 수 있다.  \n",
    "3. **Add & Norm + FFN (Eq.2)** 블록을 residual 관점에서 설명하고 구현할 수 있다.  \n",
    "4. **Encoder / Decoder의 3가지 Attention**(self / cross / masked self)을 코드 흐름으로 구분할 수 있다.  \n",
    "5. **Source mask / Target(causal) mask**가 왜 필요한지, 어디에 적용되는지 코드로 설명할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8f3ef",
   "metadata": {},
   "source": [
    "## (헷갈림 방지) CheatSheet ↔ 노트북 매핑\n",
    "\n",
    "- **Eq.(1) Attention(Q,K,V)** ↔ `SelfAttention.forward`의  \n",
    "  `attention_logits_QK` → `attention_weights` → `attention_out`\n",
    "- **Multi-Head** ↔ `values/keys/queries`를 head로 쪼개는 `reshape/permute` + 마지막 `W_O` projection\n",
    "- **Eq.(2) FFN** ↔ `TransformerBlock.forward`의 `feed_forward_out`\n",
    "- **Fig.2 Encoder-Decoder** ↔ `Encoder`, `DecoderBlock`, `Decoder`, `Transformer.forward`\n",
    "- **Mask** ↔ `make_src_mask`, `make_trg_mask` 그리고 attention logits에 적용되는 `masked_fill`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1809ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A from scratch implementation of Transformer network,\n",
    "following the paper Attention is all you need with a\n",
    "few minor differences. I tried to make it as clear as\n",
    "possible to understand and also went through the code\n",
    "on my youtube channel!\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df128cff",
   "metadata": {},
   "source": [
    "## 1) Scaled Dot-Product Attention (Eq.1) ↔ `SelfAttention.forward`\n",
    "## 2) Multi-Head Attention (MHA) ↔ split → attention → concat → projection\n",
    "\n",
    "- **핵심 수식(Eq.1)**:  $( \\text{softmax}(QK^T/\\sqrt{d_k})V )$\n",
    "- `attention_logits_QK`는 head별 `(query, key)` 점수표입니다.\n",
    "- `attention_weights`는 softmax 이후의 확률이며, 마지막에 `V`를 가중합해 `attention_out`을 만듭니다.\n",
    "\n",
    "### 사고 질문\n",
    "- (why) 내적 점수에 $1/\\sqrt{d_k}$ 스케일링을 넣는 이유는?  \n",
    "- (how) head로 쪼개면 표현력이 왜 늘어날까? (단일 head와 비교)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 1) SelfAttention (Scaled Dot-Product + Multi-Head)\n",
    "# - CheatSheet §1~2 | CookBook step.2, 6~12\n",
    "# - Eq.(1): softmax(QK^T / sqrt(d_k)) V\n",
    "# =========================================================\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = embed_size\n",
    "        self.h = heads\n",
    "        # [CheatSheet §2 Multi-Head Attention | Step 1/2]\n",
    "        # 힌트: transformer_cookbook.md [step.2]\n",
    "        self.d_k = self.d_model // _____________\n",
    "        # [CheatSheet §2 Multi-Head Attention | Step 2/2]\n",
    "        assert (\n",
    "            self.h * self.d_k == self.d_model\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        # [CheatSheet §1 Scaled Dot-Product Attention | Step 1/2] Q,K,V 선형변환 정의 (Eq.1)\n",
    "        # 힌트: transformer_cookbook.md [step.6]\n",
    "        # ================================================================================\n",
    "        # Multi Head Self Attention:  원래는 Q, K, V를 (d_model -> d_k) 크기로 변환하는 작업입니다.\n",
    "        # 하지만 논문의 구조를 그대로 따르려면 이 연산을 8번 해야하는데, 이러면 연산이 너무 비효율적입니다.\n",
    "        # 그렇다면 원래 사이즈로 변환을 한 다음 그것을 head의 수만큼 나눠주면 되겠죠?\n",
    "        # 이 말이 이해 되셨다면 빈칸을 채우실 수 있습니다!\n",
    "        # ================================================================================\n",
    "        self.W_V = nn.Linear(__________, ____________)  \n",
    "        self.W_K = nn.Linear(__________, ____________)  \n",
    "        self.W_Q = nn.Linear(__________, ____________)\n",
    "        # [CheatSheet §2 Multi-Head Attention | Step 2/2] Concat(heads) 이후 W^O (Fig.2)\n",
    "        # 힌트: transformer_cookbook.md [step.12]\n",
    "        self.W_O = nn.Linear(__________, ____________)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # [CheatSheet Shape 규칙 | Step 1/9] 배치 크기 N / 길이 추출\n",
    "        # ================================================================================\n",
    "        # NLP 모델에서 텐서의 shape은 보통 (배치 크기, 문장 길이, 임베딩 차원) 순서로 들어옵니다.\n",
    "        # CheatSheet Shape 규칙을 보셨다면 N이 무엇을 의미하는지 아실 겁니다.\n",
    "        # ================================================================================\n",
    "        N = ____________\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # [CheatSheet §1 Scaled Dot-Product Attention | Step 2/9] Q, K, V 만들기 (Eq.1)\n",
    "        # 힌트: transformer_cookbook.md [step.6]\n",
    "        V = self.W_V(values)\n",
    "        K = self.W_K(keys)\n",
    "        Q = self.W_Q(query)\n",
    "\n",
    "        # [CheatSheet §2 Multi-Head Attention | Step 3/9]\n",
    "        # 힌트: transformer_cookbook.md [step.7]\n",
    "        # ================================================================================\n",
    "        # 여기서는 Multi Head Attention을 위해 Q, K, V를 head 수만큼 나누는 과정입니다.\n",
    "        # 기존의 Q, K, V의 shape은 (배치 크기(N), 문장 길이(len), d_model) 입니다.\n",
    "        # 여기서 head와 관련된 연산이 수행된 변수가 하나 있습니다.\n",
    "        # 이제 빈칸을 풀어보세요!\n",
    "        # ================================================================================\n",
    "        V = V.reshape(N, value_len, ____________, ____________)  \n",
    "        K = K.reshape(N, key_len, ____________, ____________)  \n",
    "        Q = Q.reshape(N, query_len, ____________, ____________)\n",
    "\n",
    "        # [CheatSheet §1 Scaled Dot-Product Attention | Step 4/9] attention_logits = QK^T (Eq.1)\n",
    "        # 힌트: transformer_cookbook.md [step.8]  einsum(\"nqhd,nkhd->nhqk\")\n",
    "        attention_logits_QK = torch.einsum(\"____________________\", [Q, K])  # 설명: head별 QK^T 점수표\n",
    "\n",
    "        # [CheatSheet §4 Mask | Step 5/9] mask 적용 (padding/causal 차단)\n",
    "        # 힌트: transformer_cookbook.md [step.9]  masked_fill(mask==0, -inf)\n",
    "        if mask is not None:\n",
    "            attention_logits_QK = attention_logits_QK.masked_fill(__________ == 0, float(\"-1e20\"))  # 설명: softmax 후 0 되게\n",
    "\n",
    "        # [CheatSheet §1 Scaled Dot-Product Attention | Step 6/9] scale(+softmax) (Eq.1)\n",
    "        # - / sqrt(d_k) 로 softmax 포화 방지\n",
    "        # - softmax dim은 key_len 축(마지막 축)\n",
    "        # 힌트: transformer_cookbook.md [step.10]\n",
    "        attention_weights = torch.softmax(attention_logits_QK / (self.d_k ** (1 / 2)), dim=-1)\n",
    "\n",
    "        # [CheatSheet §1 Scaled Dot-Product Attention | Step 7/9] out_heads = attention_weights @ V (Eq.1)\n",
    "        # 힌트: transformer_cookbook.md [step.11]\n",
    "        out_heads = torch.einsum(\"____________________\", [attention_weights, V])  # 설명: 가중합으로 새 표현 생성\n",
    "        # [CheatSheet §2 Multi-Head Attention | Step 8/9]\n",
    "        out = out_heads.reshape(N, query_len, ____________ * ____________)  # 설명: head 축 결합\n",
    "\n",
    "        # [CheatSheet §2 Multi-Head Attention | Step 9/9] W^O output projection (Fig.2)\n",
    "        out = self.W_O(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df0143",
   "metadata": {},
   "source": [
    "### ✅ Check 1: SelfAttention output shape 테스트\n",
    "(빈칸을 채운 뒤 실행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "125b18e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SelfAttention output shape: (2, 5, 32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "N = 2\n",
    "q_len = 5\n",
    "d_model = 32\n",
    "h = 4\n",
    "\n",
    "self_attn = SelfAttention(embed_size=d_model, heads=h)\n",
    "\n",
    "query = torch.randn(N, q_len, d_model)  # (N, q_len, d_model)\n",
    "keys = query  # (N, q_len, d_model)\n",
    "values = query  # (N, q_len, d_model)\n",
    "\n",
    "mask = torch.ones(N, 1, 1, q_len)  # (N, 1, 1, k_len)\n",
    "mask[0, :, :, -1] = 0  # (N, 1, 1, k_len)\n",
    "\n",
    "out = self_attn(values=values, keys=keys, query=query, mask=mask)  # (N, q_len, d_model)\n",
    "assert out.shape == (N, q_len, d_model), f\"shape mismatch: {out.shape}\"\n",
    "assert torch.isfinite(out).all(), \"NaN/Inf detected in SelfAttention output\"\n",
    "print(\"[OK] SelfAttention output shape:\", tuple(out.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace51e2c",
   "metadata": {},
   "source": [
    "## 5) Add & Norm + FFN (Eq.2) ↔ `TransformerBlock.forward`\n",
    "\n",
    "- **Residual(Add)**: 원본 입력을 보존한 채, 변환 결과를 더합니다.\n",
    "- **LayerNorm(Norm)**: 분포를 안정화합니다.\n",
    "- **FFN(Eq.2)**:  $\\text{FFN}(x)=\\max(0, xW_1+b_1)W_2+b_2$\n",
    "\n",
    "### 사고 질문\n",
    "- (why) Attention 뒤에 바로 FFN을 한 번 더 넣는 이유는?  \n",
    "- (how) residual이 없으면 역전파에서 어떤 문제가 생길까?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00113aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 5) TransformerBlock (Add & Norm + FFN)\n",
    "# - CheatSheet §5 | CookBook step.13~14\n",
    "# =========================================================\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # [CheatSheet §5 Add & Norm + FFN | Step 1/2] \n",
    "        # 힌트: transformer_cookbook.md [step.14]\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        # [CheatSheet §5 Add & Norm + FFN | Step 1/4] (Sublayer) Multi-Head Attention\n",
    "        # 힌트: transformer_cookbook.md [step.13]\n",
    "        multihead_attention_output = self.attention(value, key, query, mask)\n",
    "\n",
    "        # [CheatSheet §5 Add & Norm + FFN | Step 2/4] Add & Norm (Residual + LayerNorm + Dropout)\n",
    "        # ================================================================================\n",
    "        # ResNet 스터디에서 공부했던 Skip Connection 기억나시죠??\n",
    "        # 물론 ResNet 논문에서 해결하려고 했던 문제는 degradation이지만, 연구 결과 Vanishing Gradient를 해결하는데도 도움이 되었다고 나옵니다.\n",
    "        # 현재 단계는 query에 대해 가장 관련있는 value를 찾는 과정이면 attention 결과에 무엇을 더해야 할지 아시겠죠?\n",
    "        # ================================================================================\n",
    "        attention_residual_add = ____________ + ____________  # 설명: residual = attention_output + query\n",
    "        post_attention_layernorm = self.norm1(attention_residual_add)\n",
    "        x = self.dropout(post_attention_layernorm)\n",
    "\n",
    "        # [CheatSheet §5 Add & Norm + FFN | Step 3/4] Position-wise FFN (shape를 보이게 쪼개서 실행)\n",
    "        # 힌트: transformer_cookbook.md [step.14]\n",
    "        ffn_linear1_output = self.feed_forward[0](x)\n",
    "        ffn_relu_output = self.feed_forward[1](ffn_linear1_output)\n",
    "        ffn_linear2_output = self.feed_forward[2](ffn_relu_output)\n",
    "\n",
    "        # [CheatSheet §5 Add & Norm + FFN | Step 4/4] Add & Norm after FFN\n",
    "        # ================================================================================\n",
    "        # ResNet 스터디에서 공부했던 Skip Connection 기억나시죠??\n",
    "        # 물론 ResNet 논문에서 해결하려고 했던 문제는 degradation이지만, 연구 결과 Vanishing Gradient를 해결하는데도 도움이 되었다고 나옵니다.\n",
    "        # 현재 단계는 x에 대해ffn 결과를 출려하는 단계이니 skip connection을 구상할 아이디어가 떠오르시죠? \n",
    "        # ================================================================================\n",
    "        ffn_residual_add = ____________ + ____________  # 설명: residual = ffn_output + x\n",
    "        post_ffn_layernorm = self.norm2(ffn_residual_add)\n",
    "        out = self.dropout(post_ffn_layernorm)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f685ed",
   "metadata": {},
   "source": [
    "### ✅ Check 2: TransformerBlock output shape 테스트\n",
    "(빈칸을 채운 뒤 실행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4748499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "N = 2\n",
    "seq_len = 6\n",
    "d_model = 32\n",
    "h = 4\n",
    "\n",
    "block = TransformerBlock(embed_size=d_model, heads=h, dropout=0.0, forward_expansion=4)\n",
    "\n",
    "x = torch.randn(N, seq_len, d_model)  # (N, seq_len, d_model)\n",
    "mask = torch.ones(N, 1, 1, seq_len)  # (N, 1, 1, seq_len)\n",
    "\n",
    "out = block(value=x, key=x, query=x, mask=mask)  # (N, seq_len, d_model)\n",
    "assert out.shape == (N, seq_len, d_model), f\"shape mismatch: {out.shape}\"\n",
    "assert torch.isfinite(out).all(), \"NaN/Inf detected in TransformerBlock output\"\n",
    "print(\"[OK] TransformerBlock output shape:\", tuple(out.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571790db",
   "metadata": {},
   "source": [
    "## 6) Embedding + Positional Encoding ↔ `Encoder/Decoder.forward`\n",
    "\n",
    "- 입력 토큰을 `word_embedding`으로 **(N, seq)** → **(N, seq, d_model)** 로 바꿉니다.\n",
    "- `position_embedding`을 더해 **순서 정보**를 주입합니다.\n",
    "- 이후 dropout을 거쳐 Block stack으로 들어갑니다.\n",
    "\n",
    "### 사고 질문\n",
    "- (why) 순서 정보가 없으면 Self-Attention은 어떤 문제가 생길까?  \n",
    "- (how) 학습형 position embedding과 sinusoidal의 장단점은?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 6) Encoder (Embedding + Positional Encoding + Encoder Stack)\n",
    "# - CheatSheet §6, §3(Encoder Self-Attention) | CookBook step.1, 4~5, 15\n",
    "# =========================================================\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src_token_ids, src_padding_mask):\n",
    "        # [CheatSheet §0 Shape 규칙 | Step 1/3] src_token_ids: (N, src_len)\n",
    "        # 힌트: transformer_cookbook.md [step.1]\n",
    "        N, src_len = src_token_ids.shape\n",
    "\n",
    "        # [CheatSheet §6 Embedding + Positional Encoding | Step 2/3] positions 만들기\n",
    "        # 힌트: transformer_cookbook.md [step.5]  arange(0, L).expand(N, L)\n",
    "        # ================================================================================\n",
    "        # Positional Encoding을 알고 있다면 \"힌트!\"부터 읽으셔도 됩니다.\n",
    "        # RNN은 본질적으로 단어가 순서대로 들어오기 때문에 장기의존성 문제가 생깁니다. 이 문제를 해결하기 위해 Transformer는 모든 단어를 한 번에 병렬연산합니다.\n",
    "        # 하지만 이 경우에는 \"나 너 좋아해\"나 \"너 나 좋아해\"나 같은 문장이 됩니다. 따라서 문장의 \"토큰\"마다 고유한 위치정보를 더해야 합니다.\n",
    "\n",
    "        # 힌트!\n",
    "        # 1. arange(0, x)를 사용해 [0, 1, 2, ..., \"문장길이\"-1] 형태의 1차원 번호표를 만듭니다.\n",
    "        # 2. expand(x, y)를 사용해 이 번호표를 y만큼 복사합니다. \n",
    "        # 즉, 텐서 모양을 (0, x) -> (x, y)로 확장하여 모든 문장에게 번호표를 나눠줍니다.\n",
    "        # 그렇다면 빈칸을 어떻게 채워야 할까요?\n",
    "        # ================================================================================\n",
    "        \n",
    "        position_ids = torch.arange(0, ____________).to(self.device)  \n",
    "        positions = position_ids.expand(__________, ____________)  \n",
    "\n",
    "        # [CheatSheet §6 Embedding + Positional Encoding | Step 3/3] token_emb + pos_emb (+ dropout)\n",
    "        # 힌트: transformer_cookbook.md [step.4~5]\n",
    "        token_embedding_d_model = self.word_embedding(src_token_ids)\n",
    "        positional_embedding_d_model = self.position_embedding(positions)\n",
    "        out = self.dropout(____________ + ____________) \n",
    "   \n",
    "\n",
    "        # [CheatSheet §3 Encoder Self-Attention | Step 1/1] Encoder stack 반복\n",
    "        # 힌트: transformer_cookbook.md [step.15]  layer(out,out,out,src_mask)\n",
    "        # ================================================================================\n",
    "        # 여기서 layer는 self.layers의 TransformerBlock 객체입니다.\n",
    "        # 아마 많은 분들이 놓치실텐데, torch의 객체를 불러오는 것만으로도 객체 내의 forward 함수는 실행됩니다.\n",
    "        # 위의 두 문장을 연결해서 생각해보세요. 그러면 아래 4개의 입력은 Transformer의 어떤 함수를 실행시키기 위해 필요할까요?\n",
    "        # 질문에 답을 할 수 있다면 빈칸을 채우실 수 있습니다!\n",
    "        # ================================================================================\n",
    "        for layer in self.layers:\n",
    "            out = layer(____________, ____________, ____________, ____________)  \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d216ae",
   "metadata": {},
   "source": [
    "### ✅ Check 3: Encoder output shape 테스트\n",
    "(빈칸을 채운 뒤 실행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "N = 2\n",
    "src_len = 7\n",
    "src_vocab_size = 50\n",
    "src_pad_idx = 0\n",
    "d_model = 32\n",
    "h = 4\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    embed_size=d_model,\n",
    "    num_layers=2,\n",
    "    heads=h,\n",
    "    device=device,\n",
    "    forward_expansion=4,\n",
    "    dropout=0.0,\n",
    "    max_length=100,\n",
    ").to(device)\n",
    "\n",
    "src_token_ids = torch.randint(1, src_vocab_size, (N, src_len)).to(device)  # (N, src_len)\n",
    "src_token_ids[0, -2:] = src_pad_idx  # (N, src_len)\n",
    "\n",
    "src_padding_mask = (src_token_ids != src_pad_idx).unsqueeze(1).unsqueeze(2)  # (N, 1, 1, src_len)\n",
    "\n",
    "enc_out = encoder(src_token_ids=src_token_ids, src_padding_mask=src_padding_mask)  # (N, src_len, d_model)\n",
    "assert enc_out.shape == (N, src_len, d_model), f\"shape mismatch: {enc_out.shape}\"\n",
    "assert torch.isfinite(enc_out).all(), \"NaN/Inf detected in Encoder output\"\n",
    "print(\"[OK] Encoder output shape:\", tuple(enc_out.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd39888",
   "metadata": {},
   "source": [
    "## 3) Encoder / Decoder의 3가지 Attention 매핑 ↔ `DecoderBlock.forward`\n",
    "\n",
    "DecoderBlock에는 보통 2개의 attention이 있습니다.\n",
    "\n",
    "1. **Masked Self-Attention**: Decoder 내부에서 미래 토큰을 못 보게 causal mask 적용  \n",
    "2. **Cross-Attention(Encoder-Decoder Attention)**: Query는 decoder, Key/Value는 encoder output  \n",
    "3. (Encoder는) **Self-Attention**만 사용\n",
    "\n",
    "### 사고 질문\n",
    "- (why) decoder self-attention에는 반드시 causal mask가 필요할까?  \n",
    "- (how) cross-attention에서 Q/K/V의 출처를 코드로 정확히 짚어보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b6437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 3) DecoderBlock (Masked Self-Attention + Cross-Attention)\n",
    "# - CheatSheet §3 | CookBook step.16~17\n",
    "# =========================================================\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        # [CheatSheet §3 Decoder Masked Self-Attention | Step 1/3] masked self-attn (미래 토큰 차단)\n",
    "        # 힌트: transformer_cookbook.md [step.16]\n",
    "        # ================================================================================\n",
    "        # 코드 내에서 self.attention은 SelfAttention 객체이니 이 객체를 호출했다면 forward 함수를 실행해야 합니다.\n",
    "        # 그리고 여기는 Decoder이므로 Masked Multi Head Self Attention을 해야 합니다.\n",
    "        # 그러면 어디를 찾아야 할지 아시겠죠??????????\n",
    "        # ================================================================================\n",
    "        \n",
    "        masked_self_attention_output = self.attention(____________, ____________, ____________, ____________)  \n",
    "\n",
    "        # [CheatSheet §5 Add & Norm | Step 2/3] Residual + LayerNorm + Dropout (query 만들기)\n",
    "        residual_add = ____________ + ____________  \n",
    "        query = self.dropout(self.norm(residual_add))\n",
    "\n",
    "        # [CheatSheet §3 Encoder-Decoder Attention | Step 3/3] Cross-Attention(+FFN) via TransformerBlock\n",
    "        # 힌트: transformer_cookbook.md [step.17] \n",
    "        out = self.transformer_block(____________, ____________, ____________, ____________)  \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88bf03",
   "metadata": {},
   "source": [
    "### ✅ Check 4: DecoderBlock output shape 테스트\n",
    "(빈칸을 채운 뒤 실행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98844450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "N = 2\n",
    "src_len = 7\n",
    "trg_len = 5\n",
    "d_model = 32\n",
    "h = 4\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "decoder_block = DecoderBlock(\n",
    "    embed_size=d_model,\n",
    "    heads=h,\n",
    "    forward_expansion=4,\n",
    "    dropout=0.0,\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "x = torch.randn(N, trg_len, d_model).to(device)  # (N, trg_len, d_model)\n",
    "value = torch.randn(N, src_len, d_model).to(device)  # (N, src_len, d_model)\n",
    "key = value  # (N, src_len, d_model)\n",
    "\n",
    "src_mask = torch.ones(N, 1, 1, src_len).to(device)  # (N, 1, 1, src_len)\n",
    "\n",
    "trg_mask_base = torch.tril(torch.ones(trg_len, trg_len)).to(device)  # (trg_len, trg_len)\n",
    "trg_mask = trg_mask_base.expand(N, 1, trg_len, trg_len)  # (N, 1, trg_len, trg_len)\n",
    "\n",
    "out = decoder_block(x=x, value=value, key=key, src_mask=src_mask, trg_mask=trg_mask)  # (N, trg_len, d_model)\n",
    "assert out.shape == (N, trg_len, d_model), f\"shape mismatch: {out.shape}\"\n",
    "assert torch.isfinite(out).all(), \"NaN/Inf detected in DecoderBlock output\"\n",
    "print(\"[OK] DecoderBlock output shape:\", tuple(out.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18e108",
   "metadata": {},
   "source": [
    "## 6) Embedding + Positional Encoding ↔ `Encoder/Decoder.forward`\n",
    "\n",
    "- 입력 토큰을 `word_embedding`으로 **(N, seq)** → **(N, seq, d_model)** 로 바꿉니다.\n",
    "- `position_embedding`을 더해 **순서 정보**를 주입합니다.\n",
    "- 이후 dropout을 거쳐 Block stack으로 들어갑니다.\n",
    "\n",
    "### 사고 질문\n",
    "- (why) 순서 정보가 없으면 Self-Attention은 어떤 문제가 생길까?  \n",
    "- (how) 학습형 position embedding과 sinusoidal의 장단점은?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 6) Decoder (Embedding + Positional Encoding + Decoder Stack + Vocab Projection)\n",
    "# - CheatSheet §6, §7 | CookBook step.1, 4~5, 18\n",
    "# =========================================================\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg_token_ids, enc_out, src_padding_mask, trg_causal_mask):\n",
    "        # [CheatSheet §0 Shape 규칙 | Step 1/4] trg_token_ids: (N, trg_len)\n",
    "        # 힌트: transformer_cookbook.md [step.1]\n",
    "        N, trg_len = trg_token_ids.shape\n",
    "\n",
    "        # [CheatSheet §6 Embedding + Positional Encoding | Step 2/4] positions 만들기\n",
    "        # 힌트: transformer_cookbook.md [step.5]\n",
    "        position_ids = torch.arange(0, ____________).to(self.device) \n",
    "        positions = position_ids.expand(__________, ____________) \n",
    "\n",
    "        # [CheatSheet §6 Embedding + Positional Encoding | Step 3/4] token_emb + pos_emb (+ dropout)\n",
    "        token_embedding_d_model = self.word_embedding(trg_token_ids)\n",
    "        positional_embedding_d_model = self.position_embedding(positions)\n",
    "        x = self.dropout(____________ + ____________) \n",
    "\n",
    "        # [CheatSheet §3 Decoder Stack | Step 1/1] DecoderBlock 반복\n",
    "        # 힌트: transformer_cookbook.md [step.18]\n",
    "        for layer in self.layers:\n",
    "            x = layer(____________, ____________, ____________, ____________, ____________) \n",
    "\n",
    "        # [CheatSheet §7 Output projection | Step 4/4] vocab logits 생성\n",
    "        # 힌트: transformer_cookbook.md [step.18] \n",
    "        out = self.fc_out(____________)  \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea675b1e",
   "metadata": {},
   "source": [
    "## 4) Mask(마스크) ↔ `make_src_mask`, `make_trg_mask`\n",
    "\n",
    "- **Source mask**: PAD 토큰을 attention에서 무시하기 위함 (`src_pad_idx`)\n",
    "- **Target mask(causal)**: 미래 토큰을 보지 못하게 하는 **상삼각 마스크**\n",
    "\n",
    "적용 위치: attention logits에 `masked_fill(mask==0, -1e20)` 처럼 큰 음수로 가려 softmax 후 0이 되게 합니다.\n",
    "\n",
    "### 사고 질문\n",
    "- (why) logits 단계에서 마스킹해야 softmax 후 정확히 0이 될까?  \n",
    "- (how) target mask의 shape를 head/배치 차원까지 맞추는 흐름을 추적해보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 0) Transformer (Encoder + Decoder + Mask 2종)\n",
    "# - CheatSheet §0(전체), §4(Mask) | CookBook step.3, 19\n",
    "# =========================================================\n",
    "# 이 cell은 주석을 추가로 채울 방벙을 잘 모르겠어. 나부터 이 코드를 잘 모르겠어서 이해하는 것부터 도와줘.\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device=\"cpu\",\n",
    "        max_length=100,\n",
    "    ):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src_token_ids):\n",
    "        # [CheatSheet §4 Mask | Step 1/2] src padding mask: (src != pad) -> unsqueeze(1)->unsqueeze(2)\n",
    "        # 힌트: transformer_cookbook.md [step.3]\n",
    "        # ================================================================================\n",
    "        # 이거 테스트로 풀어볼 때 많이 어려웠습니다ㅜㅜ. 일단 변수 이름부터 읽어볼까요?\n",
    "        # \"src_is_not_pad\"는 src(원본)에서 is_not_pad(패딩이 아닌 부분)입니다. 그렇다면 src_token과 \"무엇\"이 같지 않은 부분을 확인해야 할까요?\n",
    "        # ================================================================================\n",
    "        src_is_not_pad = (src_token_ids != ____________)  # 설명: pad 토큰 위치(False)를 만들기\n",
    "        src_padding_mask = src_is_not_pad.unsqueeze(____)  \n",
    "        src_padding_mask = src_padding_mask.unsqueeze(____)  \n",
    "        return src_padding_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg_token_ids):\n",
    "        # [CheatSheet §4 Mask | Step 2/2] trg causal mask: tril(ones(L,L)).expand(N, 1, L, L)\n",
    "        # 힌트: transformer_cookbook.md [step.3]\n",
    "        N, trg_len = trg_token_ids.shape\n",
    "        trg_ones = torch.ones((__________, ____________))  # 설명: (trg_len, trg_len) ones\n",
    "        trg_lower_triangular = torch.tril(trg_ones)  # 설명: 하삼각(미래 토큰 차단)\n",
    "        trg_causal_mask = trg_lower_triangular.expand(__________, 1, ____________, ____________)  \n",
    "\n",
    "        return trg_causal_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src_token_ids, trg_token_ids):\n",
    "        # [CheatSheet 전체 구조 | Step 1/3] mask 만들기\n",
    "        # 힌트: transformer_cookbook.md [step.19]\n",
    "        src_padding_mask = self.make_src_mask(____________)\n",
    "        trg_causal_mask = self.make_trg_mask(____________)\n",
    "\n",
    "        # [CheatSheet 전체 구조 | Step 2/3] Encoder\n",
    "        # ================================================================================\n",
    "        # 객체를 호출하면 객체 안의 어떤 함수가 자동으로 호출된다고 정말 많이 남겨뒀습니다.\n",
    "        # 그러면 이제 찾아봅시다!\n",
    "        # ================================================================================\n",
    "        enc_src = self.encoder(____________, ____________)\n",
    "        # [CheatSheet 전체 구조 | Step 3/3] Decoder\n",
    "        out = self.decoder(____________, ____________, ____________, ____________)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f27889",
   "metadata": {},
   "source": [
    "### ✅ Check 5: Transformer forward + mask shape 테스트\n",
    "(빈칸을 채운 뒤 실행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6125b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "N = 2\n",
    "src_len = 8\n",
    "trg_len = 6\n",
    "\n",
    "src_vocab_size = 100\n",
    "trg_vocab_size = 120\n",
    "\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    trg_vocab_size=trg_vocab_size,\n",
    "    src_pad_idx=src_pad_idx,\n",
    "    trg_pad_idx=trg_pad_idx,\n",
    "    embed_size=32,\n",
    "    num_layers=2,\n",
    "    forward_expansion=4,\n",
    "    heads=4,\n",
    "    dropout=0.0,\n",
    "    device=device,\n",
    "    max_length=100,\n",
    ").to(device)\n",
    "\n",
    "src_token_ids = torch.randint(1, src_vocab_size, (N, src_len)).to(device)  # (N, src_len)\n",
    "trg_token_ids = torch.randint(1, trg_vocab_size, (N, trg_len)).to(device)  # (N, trg_len)\n",
    "\n",
    "src_token_ids[0, -2:] = src_pad_idx  # (N, src_len)\n",
    "trg_token_ids[0, -1] = trg_pad_idx  # (N, trg_len)\n",
    "\n",
    "trg_input_ids = trg_token_ids[:, :-1]  # (N, trg_len-1)\n",
    "trg_target_ids = trg_token_ids[:, 1:]  # (N, trg_len-1)\n",
    "\n",
    "src_padding_mask = model.make_src_mask(src_token_ids)  # (N, 1, 1, src_len)\n",
    "trg_causal_mask = model.make_trg_mask(trg_input_ids)  # (N, 1, trg_len-1, trg_len-1)\n",
    "\n",
    "assert src_padding_mask.shape == (N, 1, 1, src_len), f\"src_mask shape mismatch: {src_padding_mask.shape}\"\n",
    "assert trg_causal_mask.shape == (N, 1, trg_len - 1, trg_len - 1), f\"trg_mask shape mismatch: {trg_causal_mask.shape}\"\n",
    "\n",
    "logits = model(src_token_ids, trg_input_ids)  # (N, trg_len-1, trg_vocab_size)\n",
    "assert logits.shape == (N, trg_len - 1, trg_vocab_size), f\"logits shape mismatch: {logits.shape}\"\n",
    "assert torch.isfinite(logits).all(), \"NaN/Inf detected in Transformer logits\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "\n",
    "logits_2d = logits.reshape(-1, trg_vocab_size)  # (N*(trg_len-1), trg_vocab_size)\n",
    "targets_1d = trg_target_ids.reshape(-1)  # (N*(trg_len-1),)\n",
    "\n",
    "loss = criterion(logits_2d, targets_1d)\n",
    "loss.backward()\n",
    "\n",
    "print(\"[OK] Transformer forward + loss backward:\", loss.detach().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da7a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
    "        device\n",
    "    )\n",
    "    # (N, src_len)\n",
    "    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "    # (N, trg_len)\n",
    "\n",
    "    src_pad_idx = 0\n",
    "    trg_pad_idx = 0\n",
    "    src_vocab_size = 10\n",
    "    trg_vocab_size = 10\n",
    "    model = Transformer(\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        device=device,\n",
    "    ).to(device)\n",
    "\n",
    "    trg_input_ids = trg[:, :-1]\n",
    "    # (N, trg_len-1)\n",
    "    out = model(x, trg_input_ids)\n",
    "    # (N, trg_len-1, trg_vocab_size)\n",
    "    print(out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cd922",
   "metadata": {},
   "source": [
    "### ✅ Check 6: (데이터 다운로드 없이) 더미 배치로 **1 step 학습 루프** 스모크 테스트\n",
    "\n",
    "아래 코드는 **데이터셋 없이도** Transformer 학습 루프가 돌아가는지 확인하는 최소 테스트입니다.\n",
    "\n",
    "- **Teacher Forcing 시프트 패턴(표준):**  \n",
    "  `trg_input = trg[:, :-1]` / `trg_y = trg[:, 1:]`\n",
    "- **CrossEntropy reshape 패턴(표준):**  \n",
    "  `logits: (N, T, V) -> (N*T, V)` / `trg_y: (N, T) -> (N*T)`\n",
    "- 위 빈칸(`SelfAttention/Block/Encoder/Decoder/...`)을 다 채운 뒤 실행하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ff444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- tiny toy vocab ---\n",
    "src_vocab_size = 50\n",
    "trg_vocab_size = 60\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "\n",
    "# model hyperparams (paper-base 느낌, but tiny for smoke test)\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    trg_vocab_size=trg_vocab_size,\n",
    "    src_pad_idx=src_pad_idx,\n",
    "    trg_pad_idx=trg_pad_idx,\n",
    "    embed_size=128,\n",
    "    num_layers=2,\n",
    "    forward_expansion=4,\n",
    "    heads=4,\n",
    "    dropout=0.1,\n",
    "    device=device,\n",
    "    max_length=64,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- dummy batch (variable lengths + padding) ---\n",
    "src_batch = [\n",
    "    torch.tensor([1, 5, 6, 4, 3, 9, 2]),\n",
    "    torch.tensor([1, 8, 7, 3, 4, 5]),\n",
    "]\n",
    "trg_batch = [\n",
    "    torch.tensor([1, 7, 4, 3, 5, 9, 2, 2]),\n",
    "    torch.tensor([1, 5, 6, 2, 4, 7]),\n",
    "]\n",
    "\n",
    "src = pad_sequence(src_batch, batch_first=True, padding_value=src_pad_idx).to(device)\n",
    "# (N, src_len)\n",
    "trg = pad_sequence(trg_batch, batch_first=True, padding_value=trg_pad_idx).to(device)\n",
    "# (N, trg_len)\n",
    "\n",
    "trg_input = trg[:, :-1]\n",
    "# (N, trg_len-1)\n",
    "trg_y = trg[:, 1:]\n",
    "# (N, trg_len-1)\n",
    "\n",
    "logits = model(src, trg_input)\n",
    "# (N, trg_len-1, trg_vocab_size)\n",
    "\n",
    "logits_flat = logits.reshape(-1, logits.size(-1))\n",
    "# (N*(trg_len-1), trg_vocab_size)\n",
    "trg_y_flat = trg_y.reshape(-1)\n",
    "# (N*(trg_len-1),)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss = criterion(logits_flat, trg_y_flat)\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "optimizer.step()\n",
    "\n",
    "print(\"✅ smoke loss:\", float(loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f588248",
   "metadata": {},
   "source": [
    "## 8) 실습: Multi30k 학습을 **실제로** 돌려보기 (논문 설정 정렬)\n",
    "\n",
    "아래 코드는 `seq2seq_transformer.py`의 흐름을 가져오되, **이 노트북의 Transformer 구현(Encoder–Decoder, Fig.2)** 을 그대로 사용하도록 재구성한  \n",
    "**Multi30k(De→En) 학습 스켈레톤**입니다.\n",
    "\n",
    "- 이 섹션은 **빈칸이 없습니다.**  \n",
    "  (단, **위의 빈칸 구현이 완료되어야** 실행됩니다.)\n",
    "- Multi30k는 논문에서 사용한 WMT14보다 훨씬 작은 데이터라 **논문 BLEU를 그대로 기대하면 안 됩니다.**\n",
    "- 그래도 아래 “훈련 레시피”는 가능한 한 논문 설명에 맞춰 정렬했습니다.\n",
    "\n",
    "### 논문 정렬 포인트(훈련 레시피)\n",
    "- Optimizer: **Adam(β1=0.9, β2=0.98, ε=1e-9)**\n",
    "- Learning rate schedule: **Noam warmup(4000) + inverse sqrt decay**\n",
    "- Regularization: **dropout=0.1**, **label smoothing=0.1**(PAD는 ignore)\n",
    "\n",
    "### 이 노트북 구현 ↔ 논문 차이(중요)\n",
    "- (Paper) **Sinusoidal Positional Encoding** vs (Here) **Learned positional embedding**\n",
    "- (Paper) **BPE/word-piece** vs (Here) **spaCy word tokenizer**\n",
    "- (Paper) **weight tying**(임베딩/출력 가중치 공유) vs (Here) 미적용\n",
    "\n",
    "### 필요한 패키지(처음 1회)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69902887",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets sacrebleu spacy\n",
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b819df",
   "metadata": {},
   "source": [
    "> ✅ 처음에는 `num_epochs=1`로 “동작 확인”만 하고, 그 다음 epochs를 늘리세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce25268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_multi30k.py (paper-aligned-ish) — notebook friendly\n",
    "# - 기반: seq2seq_transformer.py\n",
    "# - 변경: (1) torchtext legacy 대신 HF datasets 사용 (Multi30k 다운로드/로딩 안정)\n",
    "#        (2) 이 노트북의 Transformer(from scratch) 그대로 사용\n",
    "#        (3) 논문 훈련 레시피(Adam betas/eps, Noam LR, label smoothing) 반영\n",
    "\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import spacy\n",
    "\n",
    "# --- optional: BLEU (sacrebleu) ---\n",
    "try:\n",
    "    import sacrebleu\n",
    "except Exception:\n",
    "    sacrebleu = None\n",
    "\n",
    "# --- HF datasets for Multi30k ---\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "except Exception:\n",
    "    load_dataset = None\n",
    "\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    \"pad\": \"<pad>\",\n",
    "    \"unk\": \"<unk>\",\n",
    "    \"sos\": \"<sos>\",\n",
    "    \"eos\": \"<eos>\",\n",
    "}\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def build_vocab(sentences, tokenize_fn, max_size: int = 10000, min_freq: int = 2):\n",
    "    \"\"\"Word-level vocab builder (toy / study-friendly).\"\"\"\n",
    "    counter = Counter()\n",
    "    for s in sentences:\n",
    "        counter.update(tokenize_fn(s))\n",
    "\n",
    "    # reserve specials at the beginning so indices are stable\n",
    "    itos = [\n",
    "        SPECIAL_TOKENS[\"pad\"],\n",
    "        SPECIAL_TOKENS[\"unk\"],\n",
    "        SPECIAL_TOKENS[\"sos\"],\n",
    "        SPECIAL_TOKENS[\"eos\"],\n",
    "    ]\n",
    "\n",
    "    for tok, freq in counter.most_common():\n",
    "        if freq < min_freq:\n",
    "            continue\n",
    "        if tok in itos:\n",
    "            continue\n",
    "        itos.append(tok)\n",
    "        if len(itos) >= max_size:\n",
    "            break\n",
    "\n",
    "    stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "\n",
    "class Multi30kWordDataset(Dataset):\n",
    "    \"\"\"(de, en) sentence pairs -> (src_ids, trg_ids)\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split,\n",
    "        src_tokenize_fn,\n",
    "        trg_tokenize_fn,\n",
    "        src_stoi,\n",
    "        trg_stoi,\n",
    "        src_max_len: int = 100,\n",
    "        trg_max_len: int = 100,\n",
    "    ):\n",
    "        self.split = split\n",
    "        self.src_tokenize_fn = src_tokenize_fn\n",
    "        self.trg_tokenize_fn = trg_tokenize_fn\n",
    "        self.src_stoi = src_stoi\n",
    "        self.trg_stoi = trg_stoi\n",
    "        self.src_max_len = src_max_len\n",
    "        self.trg_max_len = trg_max_len\n",
    "\n",
    "        self.src_unk = src_stoi[SPECIAL_TOKENS[\"unk\"]]\n",
    "        self.trg_unk = trg_stoi[SPECIAL_TOKENS[\"unk\"]]\n",
    "        self.src_sos = src_stoi[SPECIAL_TOKENS[\"sos\"]]\n",
    "        self.src_eos = src_stoi[SPECIAL_TOKENS[\"eos\"]]\n",
    "        self.trg_sos = trg_stoi[SPECIAL_TOKENS[\"sos\"]]\n",
    "        self.trg_eos = trg_stoi[SPECIAL_TOKENS[\"eos\"]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.split[idx]\n",
    "        src_text = example[\"de\"]\n",
    "        trg_text = example[\"en\"]\n",
    "\n",
    "        src_tokens = self.src_tokenize_fn(src_text)[: self.src_max_len - 2]\n",
    "        trg_tokens = self.trg_tokenize_fn(trg_text)[: self.trg_max_len - 2]\n",
    "\n",
    "        src_ids = [self.src_sos] + [self.src_stoi.get(t, self.src_unk) for t in src_tokens] + [\n",
    "            self.src_eos\n",
    "        ]\n",
    "        trg_ids = [self.trg_sos] + [self.trg_stoi.get(t, self.trg_unk) for t in trg_tokens] + [\n",
    "            self.trg_eos\n",
    "        ]\n",
    "\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(trg_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "def make_collate_fn(src_pad_idx: int, trg_pad_idx: int):\n",
    "    def collate_fn(batch):\n",
    "        src_list = [b[0] for b in batch]\n",
    "        trg_list = [b[1] for b in batch]\n",
    "        src = pad_sequence(src_list, batch_first=True, padding_value=src_pad_idx)\n",
    "        # (N, src_len)\n",
    "        trg = pad_sequence(trg_list, batch_first=True, padding_value=trg_pad_idx)\n",
    "        # (N, trg_len)\n",
    "        return src, trg\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "def noam_lr_lambda(step: int, d_model: int, warmup_steps: int = 4000):\n",
    "    \"\"\"Paper: lr = d_model^{-0.5} * min(step^{-0.5}, step * warmup^{-1.5})\"\"\"\n",
    "    step = max(step, 1)\n",
    "    return (d_model ** -0.5) * min(step ** -0.5, step * (warmup_steps ** -1.5))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_decode(\n",
    "    model,\n",
    "    src_ids_1d: torch.Tensor,\n",
    "    src_pad_idx: int,\n",
    "    trg_sos_idx: int,\n",
    "    trg_eos_idx: int,\n",
    "    max_len: int,\n",
    "    device: str,\n",
    "):\n",
    "    \"\"\"Greedy decoding for quick sanity check (not beam search).\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    src = src_ids_1d.unsqueeze(0).to(device)\n",
    "    # (N=1, src_len)\n",
    "\n",
    "    generated = [trg_sos_idx]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trg = torch.tensor(generated, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        # (N=1, trg_len)\n",
    "\n",
    "        logits = model(src, trg)\n",
    "        # (N=1, trg_len, trg_vocab_size)\n",
    "\n",
    "        next_token = int(logits[0, -1].argmax(dim=-1).item())\n",
    "        generated.append(next_token)\n",
    "\n",
    "        if next_token == trg_eos_idx:\n",
    "            break\n",
    "\n",
    "    return generated\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scheduler, criterion, device: str):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for src, trg in loader:\n",
    "        src = src.to(device)\n",
    "        # (N, src_len)\n",
    "        trg = trg.to(device)\n",
    "        # (N, trg_len)\n",
    "\n",
    "        # --- Teacher forcing shift (표준 패턴) ---\n",
    "        trg_input = trg[:, :-1]\n",
    "        # (N, trg_len-1)\n",
    "        trg_y = trg[:, 1:]\n",
    "        # (N, trg_len-1)\n",
    "\n",
    "        logits = model(src, trg_input)\n",
    "        # (N, trg_len-1, trg_vocab_size)\n",
    "\n",
    "        logits_flat = logits.reshape(-1, logits.size(-1))\n",
    "        # (N*(trg_len-1), trg_vocab_size)\n",
    "        trg_y_flat = trg_y.reshape(-1)\n",
    "        # (N*(trg_len-1),)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits_flat, trg_y_flat)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device: str):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for src, trg in loader:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        trg_input = trg[:, :-1]\n",
    "        trg_y = trg[:, 1:]\n",
    "\n",
    "        logits = model(src, trg_input)\n",
    "\n",
    "        logits_flat = logits.reshape(-1, logits.size(-1))\n",
    "        trg_y_flat = trg_y.reshape(-1)\n",
    "\n",
    "        loss = criterion(logits_flat, trg_y_flat)\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "\n",
    "def main(\n",
    "    num_epochs: int = 1,\n",
    "    batch_size: int = 64,\n",
    "    max_vocab_size: int = 10000,\n",
    "    min_freq: int = 2,\n",
    "    max_len: int = 100,\n",
    "    d_model: int = 512,\n",
    "    num_layers: int = 6,\n",
    "    num_heads: int = 8,\n",
    "    forward_expansion: int = 4,\n",
    "    dropout: float = 0.1,\n",
    "    warmup_steps: int = 4000,\n",
    "):\n",
    "    if load_dataset is None:\n",
    "        raise ImportError(\"❌ datasets가 없습니다. 먼저 `pip install datasets`를 실행하세요.\")\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    set_seed(42)\n",
    "\n",
    "    # --- Load Multi30k (bentrevett subset on HF hub) ---\n",
    "    raw = load_dataset(\"bentrevett/multi30k\")\n",
    "    train_raw = raw[\"train\"]\n",
    "    valid_raw = raw[\"validation\"]\n",
    "    test_raw = raw[\"test\"]\n",
    "\n",
    "    # --- Tokenizers (spaCy) ---\n",
    "    # spaCy v3+에서는 'de'/'en' shortcut이 아니라 full model name이 필요합니다.\n",
    "    spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def tokenize_de(text: str):\n",
    "        return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "    def tokenize_en(text: str):\n",
    "        return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    # --- Build vocab (word-level) ---\n",
    "    src_stoi, src_itos = build_vocab(train_raw[\"de\"], tokenize_de, max_size=max_vocab_size, min_freq=min_freq)\n",
    "    trg_stoi, trg_itos = build_vocab(train_raw[\"en\"], tokenize_en, max_size=max_vocab_size, min_freq=min_freq)\n",
    "\n",
    "    src_pad_idx = src_stoi[SPECIAL_TOKENS[\"pad\"]]\n",
    "    trg_pad_idx = trg_stoi[SPECIAL_TOKENS[\"pad\"]]\n",
    "    trg_sos_idx = trg_stoi[SPECIAL_TOKENS[\"sos\"]]\n",
    "    trg_eos_idx = trg_stoi[SPECIAL_TOKENS[\"eos\"]]\n",
    "\n",
    "    # --- Datasets / Loaders ---\n",
    "    train_ds = Multi30kWordDataset(train_raw, tokenize_de, tokenize_en, src_stoi, trg_stoi, max_len, max_len)\n",
    "    valid_ds = Multi30kWordDataset(valid_raw, tokenize_de, tokenize_en, src_stoi, trg_stoi, max_len, max_len)\n",
    "    test_ds = Multi30kWordDataset(test_raw, tokenize_de, tokenize_en, src_stoi, trg_stoi, max_len, max_len)\n",
    "\n",
    "    collate_fn = make_collate_fn(src_pad_idx, trg_pad_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # --- Model (this notebook's from-scratch Transformer) ---\n",
    "    model = Transformer(\n",
    "        src_vocab_size=len(src_itos),\n",
    "        trg_vocab_size=len(trg_itos),\n",
    "        src_pad_idx=src_pad_idx,\n",
    "        trg_pad_idx=trg_pad_idx,\n",
    "        embed_size=d_model,\n",
    "        num_layers=num_layers,\n",
    "        forward_expansion=forward_expansion,\n",
    "        heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        device=device,\n",
    "        max_length=max_len,\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Paper-aligned optimizer + schedule ---\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1.0, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: noam_lr_lambda(step + 1, d_model=d_model, warmup_steps=warmup_steps),\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(\n",
    "        ignore_index=trg_pad_idx,\n",
    "        label_smoothing=0.1,  # paper: ε_ls = 0.1\n",
    "    )\n",
    "\n",
    "    # --- Train ---\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, criterion, device)\n",
    "        valid_loss = evaluate(model, valid_loader, criterion, device)\n",
    "\n",
    "        print(f\"epoch={epoch:02d} train_loss={train_loss:.4f} valid_loss={valid_loss:.4f}\")\n",
    "\n",
    "        # quick qualitative check: translate a random validation sample\n",
    "        sample = valid_raw[random.randrange(len(valid_raw))]\n",
    "        src_text = sample[\"de\"]\n",
    "        trg_text = sample[\"en\"]\n",
    "\n",
    "        src_tokens = tokenize_de(src_text)[: max_len - 2]\n",
    "        src_ids = [src_stoi[SPECIAL_TOKENS[\"sos\"]]] + [src_stoi.get(t, src_stoi[SPECIAL_TOKENS[\"unk\"]]) for t in src_tokens] + [src_stoi[SPECIAL_TOKENS[\"eos\"]]]\n",
    "        src_ids = torch.tensor(src_ids, dtype=torch.long)\n",
    "\n",
    "        pred_ids = greedy_decode(\n",
    "            model=model,\n",
    "            src_ids_1d=src_ids,\n",
    "            src_pad_idx=src_pad_idx,\n",
    "            trg_sos_idx=trg_sos_idx,\n",
    "            trg_eos_idx=trg_eos_idx,\n",
    "            max_len=50,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        pred_tokens = [trg_itos[i] for i in pred_ids]\n",
    "        pred_tokens = [t for t in pred_tokens if t not in {SPECIAL_TOKENS[\"sos\"], SPECIAL_TOKENS[\"eos\"], SPECIAL_TOKENS[\"pad\"]}]\n",
    "\n",
    "        print(\"DE:\", src_text)\n",
    "        print(\"GT:\", trg_text)\n",
    "        print(\"PR:\", \" \".join(pred_tokens))\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    # --- BLEU (optional) ---\n",
    "    if sacrebleu is None:\n",
    "        print(\"sacrebleu가 없어서 BLEU를 생략합니다. (pip install sacrebleu)\")\n",
    "        return\n",
    "\n",
    "    # quick BLEU on a small subset (speed)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    refs = []\n",
    "\n",
    "    for i in range(200):\n",
    "        ex = test_raw[i]\n",
    "        src_text = ex[\"de\"]\n",
    "        ref_text = ex[\"en\"]\n",
    "\n",
    "        src_tokens = tokenize_de(src_text)[: max_len - 2]\n",
    "        src_ids = [src_stoi[SPECIAL_TOKENS[\"sos\"]]] + [src_stoi.get(t, src_stoi[SPECIAL_TOKENS[\"unk\"]]) for t in src_tokens] + [src_stoi[SPECIAL_TOKENS[\"eos\"]]]\n",
    "        src_ids = torch.tensor(src_ids, dtype=torch.long)\n",
    "\n",
    "        pred_ids = greedy_decode(\n",
    "            model=model,\n",
    "            src_ids_1d=src_ids,\n",
    "            src_pad_idx=src_pad_idx,\n",
    "            trg_sos_idx=trg_sos_idx,\n",
    "            trg_eos_idx=trg_eos_idx,\n",
    "            max_len=50,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        pred_tokens = [trg_itos[i] for i in pred_ids]\n",
    "        pred_tokens = [t for t in pred_tokens if t not in {SPECIAL_TOKENS[\"sos\"], SPECIAL_TOKENS[\"eos\"], SPECIAL_TOKENS[\"pad\"]}]\n",
    "        preds.append(\" \".join(pred_tokens))\n",
    "        refs.append(ref_text)\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(preds, [refs]).score\n",
    "    print(f\"BLEU (greedy, first 200 test samples) = {bleu:.2f}\")\n",
    "\n",
    "\n",
    "# ✅ 실행 예시 (처음엔 epochs를 줄여서!)\n",
    "main(num_epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe108c6",
   "metadata": {},
   "source": [
    "## (정답 공개) — 정말 마지막에만 확인하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea9aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# ANSWERS — 빈칸에 들어갈 내용만 (마지막에 확인하세요)\n",
    "# ===========================\n",
    "\n",
    "# --- SelfAttention ---\n",
    "# [__init__]\n",
    "# self.d_k = self.d_model // self.h\n",
    "# self.d_k * self.h == self.d_model\n",
    "# self.W_V = nn.Linear(self.d_model, self.d_model)\n",
    "# self.W_K = nn.Linear(self.d_model, self.d_model)\n",
    "# self.W_Q = nn.Linear(self.d_model, self.d_model)\n",
    "# self.W_O = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "# [forward]\n",
    "# N = query.shape[0]\n",
    "# V = V.reshape(N, value_len, self.h, self.d_k)  # (N, value_len, h, d_k)\n",
    "# K = K.reshape(N, key_len, self.h, self.d_k)  # (N, key_len, h, d_k)\n",
    "# Q = Q.reshape(N, query_len, self.h, self.d_k)  # (N, query_len, h, d_k)\n",
    "# attention_logits_QK = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K])\n",
    "# attention_logits_QK = attention_logits_QK.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "# attention_weights = torch.softmax(attention_logits_QK / (self.d_k ** (1 / 2)), dim=3)\n",
    "# out_heads = torch.einsum(\"nhql,nlhd->nqhd\", [attention_weights, V])\n",
    "# out = out_heads.reshape(N, query_len, self.h * self.d_k)\n",
    "\n",
    "\n",
    "# --- TransformerBlock ---\n",
    "# [forward]\n",
    "# attention_residual_add = multihead_attention_output + query\n",
    "# ffn_residual_add = ffn_linear2_output + x\n",
    "\n",
    "\n",
    "# --- Encoder ---\n",
    "# [forward]\n",
    "# position_ids = torch.arange(0, src_len).to(self.device)\n",
    "# positions = position_ids.expand(N, src_len)\n",
    "# out = self.dropout(token_embedding_d_model + positional_embedding_d_model)\n",
    "# out = layer(out, out, out, src_padding_mask)\n",
    "\n",
    "\n",
    "# --- DecoderBlock ---\n",
    "# [forward]\n",
    "# masked_self_attention_output = self.attention(x, x, x, trg_mask)\n",
    "# residual_add = masked_self_attention_output + x\n",
    "# out = self.transformer_block(value, key, query, src_mask)\n",
    "\n",
    "\n",
    "# --- Decoder ---\n",
    "# [forward]\n",
    "# position_ids = torch.arange(0, trg_len).to(self.device)\n",
    "# positions = position_ids.expand(N, trg_len)\n",
    "# x = self.dropout(token_embedding_d_model + positional_embedding_d_model)\n",
    "# x = layer(x, enc_out, enc_out, src_padding_mask, trg_causal_mask)\n",
    "# out = self.fc_out(x)\n",
    "\n",
    "\n",
    "# --- Transformer ---\n",
    "# [make_src_mask]\n",
    "# src_is_not_pad = (src_token_ids != self.src_pad_idx)\n",
    "# src_padding_mask = src_is_not_pad.unsqueeze(1)\n",
    "# src_padding_mask = src_padding_mask.unsqueeze(2)\n",
    "\n",
    "# [make_trg_mask]\n",
    "# trg_ones = torch.ones((trg_len, trg_len))\n",
    "# trg_causal_mask = trg_lower_triangular.expand(N, 1, trg_len, trg_len)\n",
    "\n",
    "# [forward]\n",
    "# src_padding_mask = self.make_src_mask(src_token_ids)\n",
    "# trg_causal_mask = self.make_trg_mask(trg_token_ids)\n",
    "# enc_src = self.encoder(src_token_ids, src_padding_mask)\n",
    "# out = self.decoder(trg_token_ids, enc_src, src_padding_mask, trg_causal_mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
