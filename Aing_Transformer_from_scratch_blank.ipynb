{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5278d77c",
   "metadata": {},
   "source": [
    "# Transformer from scratch — Blank Notebook (Aing)\n",
    "\n",
    "이 노트북은 `transformer_from_scratch.py`를 기반으로 **Transformer 핵심 구성요소**(Self-Attention / Multi-Head / Add&Norm / FFN / Encoder-Decoder / Mask)를  \n",
    "`____` **빈칸을 채우며** 논문 **Eq.(1), Eq.(2), Fig.2**의 흐름을 코드로 연결하는 실습 자료입니다.\n",
    "\n",
    "- 빈칸은 **이해에 핵심인 지점만** 뚫었습니다. (의미 없는 빈칸 X)\n",
    "- 각 변수명은 논문 표기와 매핑되도록 구성했습니다. (예: `Q,K,V`, `attention_logits_QK`, `attention_weights`)\n",
    "- 텐서 차원이 바뀌는 연산 직후에는 **반드시 Shape 주석**으로 검증하게 되어 있습니다.\n",
    "\n",
    "> 사용법  \n",
    "> 1) 위에서부터 내려오며 `____`만 채우기  \n",
    "> 2) Shape 주석과 동일한지 수시로 확인하기  \n",
    "> 3) 마지막 `__main__` 테스트로 end-to-end 동작 확인하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382bd87e",
   "metadata": {},
   "source": [
    "## (선택) 실행을 위한 설치\n",
    "\n",
    "이미 환경에 PyTorch/Jupyter가 있다면 건너뛰세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c256a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수: PyTorch (텐서 연산/모델 정의)\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e872e421",
   "metadata": {},
   "source": [
    "## 학습 목표 (초급→중급)\n",
    "\n",
    "1. **Scaled Dot-Product Attention (Eq.1)** 을 코드로 구현하고, `QK^T / sqrt(d_k)`의 의미를 설명할 수 있다.  \n",
    "2. **Multi-Head Attention**에서 `(N, seq, d_model)` → `(N, heads, seq, d_k)`로의 변환을 직접 구현할 수 있다.  \n",
    "3. **Add & Norm + FFN (Eq.2)** 블록을 residual 관점에서 설명하고 구현할 수 있다.  \n",
    "4. **Encoder / Decoder의 3가지 Attention**(self / cross / masked self)을 코드 흐름으로 구분할 수 있다.  \n",
    "5. **Source mask / Target(causal) mask**가 왜 필요한지, 어디에 적용되는지 코드로 설명할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8f3ef",
   "metadata": {},
   "source": [
    "## (헷갈림 방지) CheatSheet ↔ 노트북 매핑\n",
    "\n",
    "- **Eq.(1) Attention(Q,K,V)** ↔ `SelfAttention.forward`의  \n",
    "  `attention_logits_QK` → `attention_weights` → `attention_out`\n",
    "- **Multi-Head** ↔ `values/keys/queries`를 head로 쪼개는 `reshape/permute` + 마지막 `W_O` projection\n",
    "- **Eq.(2) FFN** ↔ `TransformerBlock.forward`의 `feed_forward_out`\n",
    "- **Fig.2 Encoder-Decoder** ↔ `Encoder`, `DecoderBlock`, `Decoder`, `Transformer.forward`\n",
    "- **Mask** ↔ `make_src_mask`, `make_trg_mask` 그리고 attention logits에 적용되는 `masked_fill`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A from scratch implementation of Transformer network,\n",
    "following the paper Attention is all you need with a\n",
    "few minor differences. I tried to make it as clear as\n",
    "possible to understand and also went through the code\n",
    "on my youtube channel!\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df128cff",
   "metadata": {},
   "source": [
    "## 1) Scaled Dot-Product Attention (Eq.1) ↔ `SelfAttention.forward`\n",
    "## 2) Multi-Head Attention (MHA) ↔ split → attention → concat → projection\n",
    "\n",
    "- **핵심 수식(Eq.1)**:  \\( \\text{softmax}(QK^T/\\sqrt{d_k})V \\)\n",
    "- `attention_logits_QK`는 head별 `(query, key)` 점수표입니다.\n",
    "- `attention_weights`는 softmax 이후의 확률이며, 마지막에 `V`를 가중합해 `attention_out`을 만듭니다.\n",
    "\n",
    "### 사고 질문\n",
    "- (why) 내적 점수에 `1/\\sqrt{d_k}` 스케일링을 넣는 이유는?  \n",
    "- (how) head로 쪼개면 표현력이 왜 늘어날까? (단일 head와 비교)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = embed_size\n",
    "        self.h = heads\n",
    "        # [CookBook step.2] d_k = d_model // h ; assert d_k * h == d_model\n",
    "        self.d_k = self.d_model // ________\n",
    "        assert (\n",
    "            ____________ * ____________ == ____________\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        # [CookBook step.6] Q = XW^Q, K = XW^K, V = XW^V (Eq.1)\n",
    "        self.W_V = nn.Linear(__________, ____________)\n",
    "        self.W_K = nn.Linear(__________, ____________)\n",
    "        self.W_Q = nn.Linear(__________, ____________)\n",
    "        # [CookBook step.12] W^O output projection after Concat(heads) (Fig.2)\n",
    "        self.W_O = nn.Linear(__________, ____________)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Get number of training examples\n",
    "        N = ____________\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # [CookBook step.6] Linear projections to Q, K, V\n",
    "        V = self.W_V(values)  # (N, value_len, d_model)\n",
    "        K = self.W_K(keys)  # (N, key_len, d_model)\n",
    "        Q = self.W_Q(query)  # (N, query_len, d_model)\n",
    "\n",
    "        # [CookBook step.7] Split into heads: (N, L, d_model) -> (N, L, h, d_k)\n",
    "        V = V.reshape(N, value_len, ____________, ____________)  # (N, value_len, h, d_k)\n",
    "        K = K.reshape(N, key_len, ____________, ____________)  # (N, key_len, h, d_k)\n",
    "        Q = Q.reshape(N, query_len, ____________, ____________)  # (N, query_len, h, d_k)\n",
    "\n",
    "        # [CookBook step.8] energy = QK^T (Eq.1)\n",
    "        attention_logits_QK = torch.einsum(\"____________________\", [Q, K])\n",
    "        # (N, h, query_len, key_len)\n",
    "\n",
    "        # [CookBook step.9] mask == 0 -> -inf\n",
    "        if mask is not None:\n",
    "            attention_logits_QK = attention_logits_QK.masked_fill(__________ == 0, float(\"-1e20\"))\n",
    "            # (N, h, query_len, key_len)\n",
    "\n",
    "        # [CookBook step.10] softmax( energy / sqrt(d_k), dim=key_len_axis ) (Eq.1)\n",
    "        attention_weights = torch.softmax(attention_logits_QK / (__________ ** (1 / 2)), dim=____)\n",
    "        # (N, h, query_len, key_len)\n",
    "\n",
    "        # [CookBook step.11] out = Attention(...)V (Eq.1)\n",
    "        out_heads = torch.einsum(\"____________________\", [attention_weights, V])\n",
    "        # (N, query_len, h, d_k)\n",
    "        out = out_heads.reshape(N, query_len, ____________ * ____________)\n",
    "        # (N, query_len, d_model)\n",
    "\n",
    "        # [CookBook step.12] Concat(heads) W^O\n",
    "        out = self.W_O(out)\n",
    "        # (N, query_len, d_model)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace51e2c",
   "metadata": {},
   "source": [
    "## 5) Add & Norm + FFN (Eq.2) ↔ `TransformerBlock.forward`\n",
    "\n",
    "- **Residual(Add)**: 원본 입력을 보존한 채, 변환 결과를 더합니다.\n",
    "- **LayerNorm(Norm)**: 분포를 안정화합니다.\n",
    "- **FFN(Eq.2)**:  \\( \\text{FFN}(x)=\\max(0, xW_1+b_1)W_2+b_2 \\)\n",
    "\n",
    "### 사고 질문\n",
    "- (why) Attention 뒤에 바로 FFN을 한 번 더 넣는 이유는?  \n",
    "- (how) residual이 없으면 역전파에서 어떤 문제가 생길까?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00113aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # [CookBook step.14] FFN(x)=max(0, xW1+b1)W2+b2 (Eq.2)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        # [CookBook step.13] Multi-Head Attention sub-layer\n",
    "        multihead_attention_output = self.attention(value, key, query, mask)\n",
    "        # (N, query_len, d_model)\n",
    "\n",
    "        # [CookBook step.13] Add & Norm: LayerNorm(x + Sublayer(x))\n",
    "        attention_residual_add = ____________ + ____________\n",
    "        # (N, query_len, d_model)\n",
    "        post_attention_layernorm = self.norm1(attention_residual_add)\n",
    "        # (N, query_len, d_model)\n",
    "        x = self.dropout(post_attention_layernorm)\n",
    "        # (N, query_len, d_model)\n",
    "\n",
    "        # [CookBook step.14] FFN in explicit steps (so shapes are visible)\n",
    "        ffn_linear1_output = self.feed_forward[0](x)\n",
    "        # (N, query_len, forward_expansion*d_model)\n",
    "        ffn_relu_output = self.feed_forward[1](ffn_linear1_output)\n",
    "        # (N, query_len, forward_expansion*d_model)\n",
    "        ffn_linear2_output = self.feed_forward[2](ffn_relu_output)\n",
    "        # (N, query_len, d_model)\n",
    "\n",
    "        # [CookBook step.14] Add & Norm after FFN\n",
    "        ffn_residual_add = ____________ + ____________\n",
    "        # (N, query_len, d_model)\n",
    "        post_ffn_layernorm = self.norm2(ffn_residual_add)\n",
    "        # (N, query_len, d_model)\n",
    "        out = self.dropout(post_ffn_layernorm)\n",
    "        # (N, query_len, d_model)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571790db",
   "metadata": {},
   "source": [
    "## 6) Embedding + Positional Encoding ↔ `Encoder/Decoder.forward`\n",
    "\n",
    "- 입력 토큰을 `word_embedding`으로 **(N, seq)** → **(N, seq, d_model)** 로 바꿉니다.\n",
    "- `position_embedding`을 더해 **순서 정보**를 주입합니다.\n",
    "- 이후 dropout을 거쳐 Block stack으로 들어갑니다.\n",
    "\n",
    "### 사고 질문\n",
    "- (why) 순서 정보가 없으면 Self-Attention은 어떤 문제가 생길까?  \n",
    "- (how) 학습형 position embedding과 sinusoidal의 장단점은?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src_token_ids, src_padding_mask):\n",
    "        # [CookBook step.1] src_token_ids: (N, src_len)\n",
    "        N, src_len = src_token_ids.shape\n",
    "\n",
    "        # [CookBook step.5] positions = arange(0, L).expand(N, L)\n",
    "        position_ids = torch.arange(0, ____________).to(self.device)\n",
    "        # (src_len,)\n",
    "        positions = position_ids.expand(__________, ____________)\n",
    "        # (N, src_len)\n",
    "\n",
    "        # [CookBook step.4~5] token embedding + position embedding\n",
    "        token_embedding_d_model = self.word_embedding(src_token_ids)\n",
    "        # (N, src_len, d_model)\n",
    "        positional_embedding_d_model = self.position_embedding(positions)\n",
    "        # (N, src_len, d_model)\n",
    "        out = self.dropout(____________ + ____________)\n",
    "        # (N, src_len, d_model)\n",
    "\n",
    "        # [CookBook step.15] Encoder stack: out = layer(out, out, out, src_mask)\n",
    "        for layer in self.layers:\n",
    "            out = layer(____________, ____________, ____________, ____________)\n",
    "            # (N, src_len, d_model)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd39888",
   "metadata": {},
   "source": [
    "## 3) Encoder / Decoder의 3가지 Attention 매핑 ↔ `DecoderBlock.forward`\n",
    "\n",
    "DecoderBlock에는 보통 2개의 attention이 있습니다.\n",
    "\n",
    "1. **Masked Self-Attention**: Decoder 내부에서 미래 토큰을 못 보게 causal mask 적용  \n",
    "2. **Cross-Attention(Encoder-Decoder Attention)**: Query는 decoder, Key/Value는 encoder output  \n",
    "3. (Encoder는) **Self-Attention**만 사용\n",
    "\n",
    "### 사고 질문\n",
    "- (why) decoder self-attention에는 반드시 causal mask가 필요할까?  \n",
    "- (how) cross-attention에서 Q/K/V의 출처를 코드로 정확히 짚어보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b6437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    #   - Decoder Masked Self-Attention\n",
    "    #   - Encoder-Decoder Attention (Cross-Attention)\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        # [CookBook step.16] Masked self-attention (future tokens blocked by trg_mask)\n",
    "        masked_self_attention_output = self.attention(____________, ____________, ____________, ____________)\n",
    "        # (N, trg_len, d_model)\n",
    "\n",
    "        # [CookBook step.16] Add & Norm\n",
    "        residual_add = ____________ + ____________\n",
    "        # (N, trg_len, d_model)\n",
    "        query = self.dropout(self.norm(residual_add))\n",
    "        # (N, trg_len, d_model)\n",
    "\n",
    "        # [CookBook step.17] Cross-attention + FFN via TransformerBlock(value=enc_out, key=enc_out, query=query)\n",
    "        out = self.transformer_block(____________, ____________, ____________, ____________)\n",
    "        # (N, trg_len, d_model)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18e108",
   "metadata": {},
   "source": [
    "## 6) Embedding + Positional Encoding ↔ `Encoder/Decoder.forward`\n",
    "\n",
    "- 입력 토큰을 `word_embedding`으로 **(N, seq)** → **(N, seq, d_model)** 로 바꿉니다.\n",
    "- `position_embedding`을 더해 **순서 정보**를 주입합니다.\n",
    "- 이후 dropout을 거쳐 Block stack으로 들어갑니다.\n",
    "\n",
    "### 사고 질문\n",
    "- (why) 순서 정보가 없으면 Self-Attention은 어떤 문제가 생길까?  \n",
    "- (how) 학습형 position embedding과 sinusoidal의 장단점은?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg_token_ids, enc_out, src_padding_mask, trg_causal_mask):\n",
    "        # [CookBook step.1] trg_token_ids: (N, trg_len)\n",
    "        N, trg_len = trg_token_ids.shape\n",
    "\n",
    "        # [CookBook step.5] positions = arange(0, L).expand(N, L)\n",
    "        position_ids = torch.arange(0, ____________).to(self.device)\n",
    "        # (trg_len,)\n",
    "        positions = position_ids.expand(__________, ____________)\n",
    "        # (N, trg_len)\n",
    "\n",
    "        # [CookBook step.4~5] token embedding + position embedding\n",
    "        token_embedding_d_model = self.word_embedding(trg_token_ids)\n",
    "        # (N, trg_len, d_model)\n",
    "        positional_embedding_d_model = self.position_embedding(positions)\n",
    "        # (N, trg_len, d_model)\n",
    "        x = self.dropout(____________ + ____________)\n",
    "        # (N, trg_len, d_model)\n",
    "\n",
    "        # [CookBook step.18] Decoder stack\n",
    "        for layer in self.layers:\n",
    "            x = layer(____________, ____________, ____________, ____________, ____________)\n",
    "            # (N, trg_len, d_model)\n",
    "\n",
    "        # [CookBook step.18] vocab projection -> logits\n",
    "        out = self.fc_out(____________)\n",
    "        # (N, trg_len, trg_vocab_size)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea675b1e",
   "metadata": {},
   "source": [
    "## 4) Mask(마스크) ↔ `make_src_mask`, `make_trg_mask`\n",
    "\n",
    "- **Source mask**: PAD 토큰을 attention에서 무시하기 위함 (`src_pad_idx`)\n",
    "- **Target mask(causal)**: 미래 토큰을 보지 못하게 하는 **상삼각 마스크**\n",
    "\n",
    "적용 위치: attention logits에 `masked_fill(mask==0, -1e20)` 처럼 큰 음수로 가려 softmax 후 0이 되게 합니다.\n",
    "\n",
    "### 사고 질문\n",
    "- (why) logits 단계에서 마스킹해야 softmax 후 정확히 0이 될까?  \n",
    "- (how) target mask의 shape를 head/배치 차원까지 맞추는 흐름을 추적해보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device=\"cpu\",\n",
    "        max_length=100,\n",
    "    ):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src_token_ids):\n",
    "        # [CookBook step.3] src_padding_mask = (src != pad).unsqueeze(1).unsqueeze(2)\n",
    "        src_is_not_pad = (src_token_ids != ____________)\n",
    "        # (N, src_len)\n",
    "        src_padding_mask = src_is_not_pad.unsqueeze(____)\n",
    "        # (N, 1, src_len)\n",
    "        src_padding_mask = src_padding_mask.unsqueeze(____)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_padding_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg_token_ids):\n",
    "        # [CookBook step.3] trg_causal_mask = tril(ones(L,L)).expand(N, 1, L, L)\n",
    "        N, trg_len = trg_token_ids.shape\n",
    "        trg_ones = torch.ones((__________, ____________))\n",
    "        # (trg_len, trg_len)\n",
    "        trg_lower_triangular = torch.tril(trg_ones)\n",
    "        # (trg_len, trg_len)\n",
    "        trg_causal_mask = trg_lower_triangular.expand(__________, 1, ____________, ____________)\n",
    "        # (N, 1, trg_len, trg_len)\n",
    "\n",
    "        return trg_causal_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src_token_ids, trg_token_ids):\n",
    "        # [CookBook step.19] src_mask -> encoder -> decoder\n",
    "        src_padding_mask = self.make_src_mask(____________)\n",
    "        # (N, 1, 1, src_len)\n",
    "        trg_causal_mask = self.make_trg_mask(____________)\n",
    "        # (N, 1, trg_len, trg_len)\n",
    "\n",
    "        enc_src = self.encoder(____________, ____________)\n",
    "        # (N, src_len, d_model)\n",
    "        out = self.decoder(____________, ____________, ____________, ____________)\n",
    "        # (N, trg_len, trg_vocab_size)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da7a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
    "        device\n",
    "    )\n",
    "    # (N, src_len)\n",
    "    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "    # (N, trg_len)\n",
    "\n",
    "    src_pad_idx = 0\n",
    "    trg_pad_idx = 0\n",
    "    src_vocab_size = 10\n",
    "    trg_vocab_size = 10\n",
    "    model = Transformer(\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        device=device,\n",
    "    ).to(device)\n",
    "\n",
    "    trg_input_ids = trg[:, :-1]\n",
    "    # (N, trg_len-1)\n",
    "    out = model(x, trg_input_ids)\n",
    "    # (N, trg_len-1, trg_vocab_size)\n",
    "    print(out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cd922",
   "metadata": {},
   "source": [
    "### ✅ Check 7: (데이터 다운로드 없이) 더미 배치로 **1 step 학습 루프** 스모크 테스트\n",
    "\n",
    "아래 코드는 **데이터셋 없이도** Transformer 학습 루프가 돌아가는지 확인하는 최소 테스트입니다.\n",
    "\n",
    "- **Teacher Forcing 시프트 패턴(표준):**  \n",
    "  `trg_input = trg[:, :-1]` / `trg_y = trg[:, 1:]`\n",
    "- **CrossEntropy reshape 패턴(표준):**  \n",
    "  `logits: (N, T, V) -> (N*T, V)` / `trg_y: (N, T) -> (N*T)`\n",
    "- 위 빈칸(`SelfAttention/Block/Encoder/Decoder/...`)을 다 채운 뒤 실행하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ff444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- tiny toy vocab ---\n",
    "src_vocab_size = 50\n",
    "trg_vocab_size = 60\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "\n",
    "# model hyperparams (paper-base 느낌, but tiny for smoke test)\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    trg_vocab_size=trg_vocab_size,\n",
    "    src_pad_idx=src_pad_idx,\n",
    "    trg_pad_idx=trg_pad_idx,\n",
    "    embed_size=128,\n",
    "    num_layers=2,\n",
    "    forward_expansion=4,\n",
    "    heads=4,\n",
    "    dropout=0.1,\n",
    "    device=device,\n",
    "    max_length=64,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- dummy batch (variable lengths + padding) ---\n",
    "src_batch = [\n",
    "    torch.tensor([1, 5, 6, 4, 3, 9, 2]),\n",
    "    torch.tensor([1, 8, 7, 3, 4, 5]),\n",
    "]\n",
    "trg_batch = [\n",
    "    torch.tensor([1, 7, 4, 3, 5, 9, 2, 2]),\n",
    "    torch.tensor([1, 5, 6, 2, 4, 7]),\n",
    "]\n",
    "\n",
    "src = pad_sequence(src_batch, batch_first=True, padding_value=src_pad_idx).to(device)\n",
    "# (N, src_len)\n",
    "trg = pad_sequence(trg_batch, batch_first=True, padding_value=trg_pad_idx).to(device)\n",
    "# (N, trg_len)\n",
    "\n",
    "trg_input = trg[:, :-1]\n",
    "# (N, trg_len-1)\n",
    "trg_y = trg[:, 1:]\n",
    "# (N, trg_len-1)\n",
    "\n",
    "logits = model(src, trg_input)\n",
    "# (N, trg_len-1, trg_vocab_size)\n",
    "\n",
    "logits_flat = logits.reshape(-1, logits.size(-1))\n",
    "# (N*(trg_len-1), trg_vocab_size)\n",
    "trg_y_flat = trg_y.reshape(-1)\n",
    "# (N*(trg_len-1),)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss = criterion(logits_flat, trg_y_flat)\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "optimizer.step()\n",
    "\n",
    "print(\"✅ smoke loss:\", float(loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f588248",
   "metadata": {},
   "source": [
    "## 8) 실습: Multi30k 학습을 **실제로** 돌려보기 (논문 설정 정렬)\n",
    "\n",
    "아래 코드는 `seq2seq_transformer.py`의 흐름을 가져오되, **이 노트북의 Transformer 구현(Encoder–Decoder, Fig.2)** 을 그대로 사용하도록 재구성한  \n",
    "**Multi30k(De→En) 학습 스켈레톤**입니다.\n",
    "\n",
    "- 이 섹션은 **빈칸이 없습니다.**  \n",
    "  (단, **위의 빈칸 구현이 완료되어야** 실행됩니다.)\n",
    "- Multi30k는 논문에서 사용한 WMT14보다 훨씬 작은 데이터라 **논문 BLEU를 그대로 기대하면 안 됩니다.**\n",
    "- 그래도 아래 “훈련 레시피”는 가능한 한 논문 설명에 맞춰 정렬했습니다.\n",
    "\n",
    "### 논문 정렬 포인트(훈련 레시피)\n",
    "- Optimizer: **Adam(β1=0.9, β2=0.98, ε=1e-9)**\n",
    "- Learning rate schedule: **Noam warmup(4000) + inverse sqrt decay**\n",
    "- Regularization: **dropout=0.1**, **label smoothing=0.1**(PAD는 ignore)\n",
    "\n",
    "### 이 노트북 구현 ↔ 논문 차이(중요)\n",
    "- (Paper) **Sinusoidal Positional Encoding** vs (Here) **Learned positional embedding**\n",
    "- (Paper) **BPE/word-piece** vs (Here) **spaCy word tokenizer**\n",
    "- (Paper) **weight tying**(임베딩/출력 가중치 공유) vs (Here) 미적용\n",
    "\n",
    "### 필요한 패키지(처음 1회)\n",
    "```bash\n",
    "pip install datasets sacrebleu spacy\n",
    "python -m spacy download de_core_news_sm\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "> ✅ 처음에는 `num_epochs=1`로 “동작 확인”만 하고, 그 다음 epochs를 늘리세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce25268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_multi30k.py (paper-aligned-ish) — notebook friendly\n",
    "# - 기반: seq2seq_transformer.py\n",
    "# - 변경: (1) torchtext legacy 대신 HF datasets 사용 (Multi30k 다운로드/로딩 안정)\n",
    "#        (2) 이 노트북의 Transformer(from scratch) 그대로 사용\n",
    "#        (3) 논문 훈련 레시피(Adam betas/eps, Noam LR, label smoothing) 반영\n",
    "\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import spacy\n",
    "\n",
    "# --- optional: BLEU (sacrebleu) ---\n",
    "try:\n",
    "    import sacrebleu\n",
    "except Exception:\n",
    "    sacrebleu = None\n",
    "\n",
    "# --- HF datasets for Multi30k ---\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "except Exception:\n",
    "    load_dataset = None\n",
    "\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    \"pad\": \"<pad>\",\n",
    "    \"unk\": \"<unk>\",\n",
    "    \"sos\": \"<sos>\",\n",
    "    \"eos\": \"<eos>\",\n",
    "}\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def build_vocab(sentences, tokenize_fn, max_size: int = 10000, min_freq: int = 2):\n",
    "    \"\"\"Word-level vocab builder (toy / study-friendly).\"\"\"\n",
    "    counter = Counter()\n",
    "    for s in sentences:\n",
    "        counter.update(tokenize_fn(s))\n",
    "\n",
    "    # reserve specials at the beginning so indices are stable\n",
    "    itos = [\n",
    "        SPECIAL_TOKENS[\"pad\"],\n",
    "        SPECIAL_TOKENS[\"unk\"],\n",
    "        SPECIAL_TOKENS[\"sos\"],\n",
    "        SPECIAL_TOKENS[\"eos\"],\n",
    "    ]\n",
    "\n",
    "    for tok, freq in counter.most_common():\n",
    "        if freq < min_freq:\n",
    "            continue\n",
    "        if tok in itos:\n",
    "            continue\n",
    "        itos.append(tok)\n",
    "        if len(itos) >= max_size:\n",
    "            break\n",
    "\n",
    "    stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "\n",
    "class Multi30kWordDataset(Dataset):\n",
    "    \"\"\"(de, en) sentence pairs -> (src_ids, trg_ids)\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split,\n",
    "        src_tokenize_fn,\n",
    "        trg_tokenize_fn,\n",
    "        src_stoi,\n",
    "        trg_stoi,\n",
    "        src_max_len: int = 100,\n",
    "        trg_max_len: int = 100,\n",
    "    ):\n",
    "        self.split = split\n",
    "        self.src_tokenize_fn = src_tokenize_fn\n",
    "        self.trg_tokenize_fn = trg_tokenize_fn\n",
    "        self.src_stoi = src_stoi\n",
    "        self.trg_stoi = trg_stoi\n",
    "        self.src_max_len = src_max_len\n",
    "        self.trg_max_len = trg_max_len\n",
    "\n",
    "        self.src_unk = src_stoi[SPECIAL_TOKENS[\"unk\"]]\n",
    "        self.trg_unk = trg_stoi[SPECIAL_TOKENS[\"unk\"]]\n",
    "        self.src_sos = src_stoi[SPECIAL_TOKENS[\"sos\"]]\n",
    "        self.src_eos = src_stoi[SPECIAL_TOKENS[\"eos\"]]\n",
    "        self.trg_sos = trg_stoi[SPECIAL_TOKENS[\"sos\"]]\n",
    "        self.trg_eos = trg_stoi[SPECIAL_TOKENS[\"eos\"]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.split[idx]\n",
    "        src_text = example[\"de\"]\n",
    "        trg_text = example[\"en\"]\n",
    "\n",
    "        src_tokens = self.src_tokenize_fn(src_text)[: self.src_max_len - 2]\n",
    "        trg_tokens = self.trg_tokenize_fn(trg_text)[: self.trg_max_len - 2]\n",
    "\n",
    "        src_ids = [self.src_sos] + [self.src_stoi.get(t, self.src_unk) for t in src_tokens] + [\n",
    "            self.src_eos\n",
    "        ]\n",
    "        trg_ids = [self.trg_sos] + [self.trg_stoi.get(t, self.trg_unk) for t in trg_tokens] + [\n",
    "            self.trg_eos\n",
    "        ]\n",
    "\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(trg_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "def make_collate_fn(src_pad_idx: int, trg_pad_idx: int):\n",
    "    def collate_fn(batch):\n",
    "        src_list = [b[0] for b in batch]\n",
    "        trg_list = [b[1] for b in batch]\n",
    "        src = pad_sequence(src_list, batch_first=True, padding_value=src_pad_idx)\n",
    "        # (N, src_len)\n",
    "        trg = pad_sequence(trg_list, batch_first=True, padding_value=trg_pad_idx)\n",
    "        # (N, trg_len)\n",
    "        return src, trg\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "def noam_lr_lambda(step: int, d_model: int, warmup_steps: int = 4000):\n",
    "    \"\"\"Paper: lr = d_model^{-0.5} * min(step^{-0.5}, step * warmup^{-1.5})\"\"\"\n",
    "    step = max(step, 1)\n",
    "    return (d_model ** -0.5) * min(step ** -0.5, step * (warmup_steps ** -1.5))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_decode(\n",
    "    model,\n",
    "    src_ids_1d: torch.Tensor,\n",
    "    src_pad_idx: int,\n",
    "    trg_sos_idx: int,\n",
    "    trg_eos_idx: int,\n",
    "    max_len: int,\n",
    "    device: str,\n",
    "):\n",
    "    \"\"\"Greedy decoding for quick sanity check (not beam search).\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    src = src_ids_1d.unsqueeze(0).to(device)\n",
    "    # (N=1, src_len)\n",
    "\n",
    "    generated = [trg_sos_idx]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trg = torch.tensor(generated, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        # (N=1, trg_len)\n",
    "\n",
    "        logits = model(src, trg)\n",
    "        # (N=1, trg_len, trg_vocab_size)\n",
    "\n",
    "        next_token = int(logits[0, -1].argmax(dim=-1).item())\n",
    "        generated.append(next_token)\n",
    "\n",
    "        if next_token == trg_eos_idx:\n",
    "            break\n",
    "\n",
    "    return generated\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scheduler, criterion, device: str):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for src, trg in loader:\n",
    "        src = src.to(device)\n",
    "        # (N, src_len)\n",
    "        trg = trg.to(device)\n",
    "        # (N, trg_len)\n",
    "\n",
    "        # --- Teacher forcing shift (표준 패턴) ---\n",
    "        trg_input = trg[:, :-1]\n",
    "        # (N, trg_len-1)\n",
    "        trg_y = trg[:, 1:]\n",
    "        # (N, trg_len-1)\n",
    "\n",
    "        logits = model(src, trg_input)\n",
    "        # (N, trg_len-1, trg_vocab_size)\n",
    "\n",
    "        logits_flat = logits.reshape(-1, logits.size(-1))\n",
    "        # (N*(trg_len-1), trg_vocab_size)\n",
    "        trg_y_flat = trg_y.reshape(-1)\n",
    "        # (N*(trg_len-1),)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits_flat, trg_y_flat)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device: str):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for src, trg in loader:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        trg_input = trg[:, :-1]\n",
    "        trg_y = trg[:, 1:]\n",
    "\n",
    "        logits = model(src, trg_input)\n",
    "\n",
    "        logits_flat = logits.reshape(-1, logits.size(-1))\n",
    "        trg_y_flat = trg_y.reshape(-1)\n",
    "\n",
    "        loss = criterion(logits_flat, trg_y_flat)\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "\n",
    "def main(\n",
    "    num_epochs: int = 1,\n",
    "    batch_size: int = 64,\n",
    "    max_vocab_size: int = 10000,\n",
    "    min_freq: int = 2,\n",
    "    max_len: int = 100,\n",
    "    d_model: int = 512,\n",
    "    num_layers: int = 6,\n",
    "    num_heads: int = 8,\n",
    "    forward_expansion: int = 4,\n",
    "    dropout: float = 0.1,\n",
    "    warmup_steps: int = 4000,\n",
    "):\n",
    "    if load_dataset is None:\n",
    "        raise ImportError(\"❌ datasets가 없습니다. 먼저 `pip install datasets`를 실행하세요.\")\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    set_seed(42)\n",
    "\n",
    "    # --- Load Multi30k (bentrevett subset on HF hub) ---\n",
    "    raw = load_dataset(\"bentrevett/multi30k\")\n",
    "    train_raw = raw[\"train\"]\n",
    "    valid_raw = raw[\"validation\"]\n",
    "    test_raw = raw[\"test\"]\n",
    "\n",
    "    # --- Tokenizers (spaCy) ---\n",
    "    # spaCy v3+에서는 'de'/'en' shortcut이 아니라 full model name이 필요합니다.\n",
    "    spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def tokenize_de(text: str):\n",
    "        return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "    def tokenize_en(text: str):\n",
    "        return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    # --- Build vocab (word-level) ---\n",
    "    src_stoi, src_itos = build_vocab(train_raw[\"de\"], tokenize_de, max_size=max_vocab_size, min_freq=min_freq)\n",
    "    trg_stoi, trg_itos = build_vocab(train_raw[\"en\"], tokenize_en, max_size=max_vocab_size, min_freq=min_freq)\n",
    "\n",
    "    src_pad_idx = src_stoi[SPECIAL_TOKENS[\"pad\"]]\n",
    "    trg_pad_idx = trg_stoi[SPECIAL_TOKENS[\"pad\"]]\n",
    "    trg_sos_idx = trg_stoi[SPECIAL_TOKENS[\"sos\"]]\n",
    "    trg_eos_idx = trg_stoi[SPECIAL_TOKENS[\"eos\"]]\n",
    "\n",
    "    # --- Datasets / Loaders ---\n",
    "    train_ds = Multi30kWordDataset(train_raw, tokenize_de, tokenize_en, src_stoi, trg_stoi, max_len, max_len)\n",
    "    valid_ds = Multi30kWordDataset(valid_raw, tokenize_de, tokenize_en, src_stoi, trg_stoi, max_len, max_len)\n",
    "    test_ds = Multi30kWordDataset(test_raw, tokenize_de, tokenize_en, src_stoi, trg_stoi, max_len, max_len)\n",
    "\n",
    "    collate_fn = make_collate_fn(src_pad_idx, trg_pad_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # --- Model (this notebook's from-scratch Transformer) ---\n",
    "    model = Transformer(\n",
    "        src_vocab_size=len(src_itos),\n",
    "        trg_vocab_size=len(trg_itos),\n",
    "        src_pad_idx=src_pad_idx,\n",
    "        trg_pad_idx=trg_pad_idx,\n",
    "        embed_size=d_model,\n",
    "        num_layers=num_layers,\n",
    "        forward_expansion=forward_expansion,\n",
    "        heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        device=device,\n",
    "        max_length=max_len,\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Paper-aligned optimizer + schedule ---\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1.0, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: noam_lr_lambda(step + 1, d_model=d_model, warmup_steps=warmup_steps),\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(\n",
    "        ignore_index=trg_pad_idx,\n",
    "        label_smoothing=0.1,  # paper: ε_ls = 0.1\n",
    "    )\n",
    "\n",
    "    # --- Train ---\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, criterion, device)\n",
    "        valid_loss = evaluate(model, valid_loader, criterion, device)\n",
    "\n",
    "        print(f\"epoch={epoch:02d} train_loss={train_loss:.4f} valid_loss={valid_loss:.4f}\")\n",
    "\n",
    "        # quick qualitative check: translate a random validation sample\n",
    "        sample = valid_raw[random.randrange(len(valid_raw))]\n",
    "        src_text = sample[\"de\"]\n",
    "        trg_text = sample[\"en\"]\n",
    "\n",
    "        src_tokens = tokenize_de(src_text)[: max_len - 2]\n",
    "        src_ids = [src_stoi[SPECIAL_TOKENS[\"sos\"]]] + [src_stoi.get(t, src_stoi[SPECIAL_TOKENS[\"unk\"]]) for t in src_tokens] + [src_stoi[SPECIAL_TOKENS[\"eos\"]]]\n",
    "        src_ids = torch.tensor(src_ids, dtype=torch.long)\n",
    "\n",
    "        pred_ids = greedy_decode(\n",
    "            model=model,\n",
    "            src_ids_1d=src_ids,\n",
    "            src_pad_idx=src_pad_idx,\n",
    "            trg_sos_idx=trg_sos_idx,\n",
    "            trg_eos_idx=trg_eos_idx,\n",
    "            max_len=50,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        pred_tokens = [trg_itos[i] for i in pred_ids]\n",
    "        pred_tokens = [t for t in pred_tokens if t not in {SPECIAL_TOKENS[\"sos\"], SPECIAL_TOKENS[\"eos\"], SPECIAL_TOKENS[\"pad\"]}]\n",
    "\n",
    "        print(\"DE:\", src_text)\n",
    "        print(\"GT:\", trg_text)\n",
    "        print(\"PR:\", \" \".join(pred_tokens))\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    # --- BLEU (optional) ---\n",
    "    if sacrebleu is None:\n",
    "        print(\"ℹ️ sacrebleu가 없어서 BLEU를 생략합니다. (pip install sacrebleu)\")\n",
    "        return\n",
    "\n",
    "    # quick BLEU on a small subset (speed)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    refs = []\n",
    "\n",
    "    for i in range(200):\n",
    "        ex = test_raw[i]\n",
    "        src_text = ex[\"de\"]\n",
    "        ref_text = ex[\"en\"]\n",
    "\n",
    "        src_tokens = tokenize_de(src_text)[: max_len - 2]\n",
    "        src_ids = [src_stoi[SPECIAL_TOKENS[\"sos\"]]] + [src_stoi.get(t, src_stoi[SPECIAL_TOKENS[\"unk\"]]) for t in src_tokens] + [src_stoi[SPECIAL_TOKENS[\"eos\"]]]\n",
    "        src_ids = torch.tensor(src_ids, dtype=torch.long)\n",
    "\n",
    "        pred_ids = greedy_decode(\n",
    "            model=model,\n",
    "            src_ids_1d=src_ids,\n",
    "            src_pad_idx=src_pad_idx,\n",
    "            trg_sos_idx=trg_sos_idx,\n",
    "            trg_eos_idx=trg_eos_idx,\n",
    "            max_len=50,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        pred_tokens = [trg_itos[i] for i in pred_ids]\n",
    "        pred_tokens = [t for t in pred_tokens if t not in {SPECIAL_TOKENS[\"sos\"], SPECIAL_TOKENS[\"eos\"], SPECIAL_TOKENS[\"pad\"]}]\n",
    "        preds.append(\" \".join(pred_tokens))\n",
    "        refs.append(ref_text)\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(preds, [refs]).score\n",
    "    print(f\"BLEU (greedy, first 200 test samples) = {bleu:.2f}\")\n",
    "\n",
    "\n",
    "# ✅ 실행 예시 (처음엔 epochs를 줄여서!)\n",
    "# main(num_epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe108c6",
   "metadata": {},
   "source": [
    "## (정답 공개) — 정말 마지막에만 확인하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea9aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# ANSWERS — 빈칸에 들어갈 내용만 (마지막에 확인하세요)\n",
    "# ===========================\n",
    "\n",
    "# --- SelfAttention ---\n",
    "# [__init__]\n",
    "# self.d_k = self.d_model // self.h\n",
    "# self.d_k * self.h == self.d_model\n",
    "# self.W_V = nn.Linear(self.d_model, self.d_model)\n",
    "# self.W_K = nn.Linear(self.d_model, self.d_model)\n",
    "# self.W_Q = nn.Linear(self.d_model, self.d_model)\n",
    "# self.W_O = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "# [forward]\n",
    "# N = query.shape[0]\n",
    "# V = V.reshape(N, value_len, self.h, self.d_k)  # (N, value_len, h, d_k)\n",
    "# K = K.reshape(N, key_len, self.h, self.d_k)  # (N, key_len, h, d_k)\n",
    "# Q = Q.reshape(N, query_len, self.h, self.d_k)  # (N, query_len, h, d_k)\n",
    "# attention_logits_QK = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K])\n",
    "# attention_logits_QK = attention_logits_QK.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "# attention_weights = torch.softmax(attention_logits_QK / (self.d_k ** (1 / 2)), dim=3)\n",
    "# out_heads = torch.einsum(\"nhql,nlhd->nqhd\", [attention_weights, V])\n",
    "# out = out_heads.reshape(N, query_len, self.h * self.d_k)\n",
    "\n",
    "\n",
    "# --- TransformerBlock ---\n",
    "# [forward]\n",
    "# attention_residual_add = multihead_attention_output + query\n",
    "# ffn_residual_add = ffn_linear2_output + x\n",
    "\n",
    "\n",
    "# --- Encoder ---\n",
    "# [forward]\n",
    "# position_ids = torch.arange(0, src_len).to(self.device)\n",
    "# positions = position_ids.expand(N, src_len)\n",
    "# out = self.dropout(token_embedding_d_model + positional_embedding_d_model)\n",
    "# out = layer(out, out, out, src_padding_mask)\n",
    "\n",
    "\n",
    "# --- DecoderBlock ---\n",
    "# [forward]\n",
    "# masked_self_attention_output = self.attention(x, x, x, trg_mask)\n",
    "# residual_add = masked_self_attention_output + x\n",
    "# out = self.transformer_block(value, key, query, src_mask)\n",
    "\n",
    "\n",
    "# --- Decoder ---\n",
    "# [forward]\n",
    "# position_ids = torch.arange(0, trg_len).to(self.device)\n",
    "# positions = position_ids.expand(N, trg_len)\n",
    "# x = self.dropout(token_embedding_d_model + positional_embedding_d_model)\n",
    "# x = layer(x, enc_out, enc_out, src_padding_mask, trg_causal_mask)\n",
    "# out = self.fc_out(x)\n",
    "\n",
    "\n",
    "# --- Transformer ---\n",
    "# [make_src_mask]\n",
    "# src_is_not_pad = (src_token_ids != self.src_pad_idx)\n",
    "# src_padding_mask = src_is_not_pad.unsqueeze(1)\n",
    "# src_padding_mask = src_padding_mask.unsqueeze(2)\n",
    "\n",
    "# [make_trg_mask]\n",
    "# trg_ones = torch.ones((trg_len, trg_len))\n",
    "# trg_causal_mask = trg_lower_triangular.expand(N, 1, trg_len, trg_len)\n",
    "\n",
    "# [forward]\n",
    "# src_padding_mask = self.make_src_mask(src_token_ids)\n",
    "# trg_causal_mask = self.make_trg_mask(trg_token_ids)\n",
    "# enc_src = self.encoder(src_token_ids, src_padding_mask)\n",
    "# out = self.decoder(trg_token_ids, enc_src, src_padding_mask, trg_causal_mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}